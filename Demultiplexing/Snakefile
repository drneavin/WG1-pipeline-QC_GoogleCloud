#!/usr/bin/env python
import os
import sys
import pandas as pd
from glob import glob
import subprocess
import shutil
from snakemake.remote.GS import RemoteProvider as GSRemoteProvider
from google.cloud import storage
from io import BytesIO
import re

# Import custom functions
# from mods import prepareArguments


## Define functions here instead of in mods since inconsistently includes mods folder for cache
def parsePaths(config):
    # Strip trailing slashes from paths 
    # scrnaseq_dir, individual_list_dir
    config["inputs"]["scRNAseq_dir"] = (config["inputs"]["scRNAseq_dir"]).rstrip("/")
    config["inputs"]["individual_list_dir"] = (config["inputs"]["individual_list_dir"]).rstrip("/")
    return(config)



def getFASTA(ref_dir):
    for blob in client.list_blobs(workflow.default_remote_prefix, prefix=ref_dir):
        if re.search("Homo_sapiens.GRCh38.dna.primary_assembly.fa,", str(blob)):
            return(blob.name)


def getGENEfiles(scrnaseq_libs_df):
    gene_files = []
    for blob in client.list_blobs(workflow.default_remote_prefix, prefix=scrnaseq_libs_df.Matrix_Directories.values):
        if re.search("genes.tsv,", str(blob)):
            gene_files.append((bucket.blob(scrnaseq_libs_df.Matrix_Directories.values[0]).name + "/genes.tsv"))
        elif re.search("features.tsv.gz,", str(blob)):
            gene_files.append(bucket.blob(scrnaseq_libs_df.Matrix_Directories.values).name + "/features.tsv.gz")
    return(gene_files)


def getBARCODEfiles(scrnaseq_libs_df):
    gene_files = []
    for blob in client.list_blobs(workflow.default_remote_prefix, prefix=scrnaseq_libs_df.Matrix_Directories.values):
        if re.search("genes.tsv,", str(blob)):
            gene_files.append((bucket.blob(scrnaseq_libs_df.Matrix_Directories.values[0]).name + "/barcodes.tsv"))
        elif re.search("features.tsv.gz,", str(blob)):
            gene_files.append(bucket.blob(scrnaseq_libs_df.Matrix_Directories.values).name + "/barcodes.tsv.gz")
    return(gene_files)


def getMATRIXfiles(scrnaseq_libs_df):
    gene_files = []
    for blob in client.list_blobs(workflow.default_remote_prefix, prefix=scrnaseq_libs_df.Matrix_Directories.values):
        if re.search("genes.tsv,", str(blob)):
            gene_files.append((bucket.blob(scrnaseq_libs_df.Matrix_Directories.values[0]).name + "/matrix.mtx"))
        elif re.search("features.tsv.gz,", str(blob)):
            gene_files.append(bucket.blob(scrnaseq_libs_df.Matrix_Directories.values).name + "/matrix.mtx.gz")
    return(gene_files)


### Extract variables from configuration file for use within the rest of the pipeline
configfile: config['config_file']

config = parsePaths(config)
input_dict = config["inputs"]
ref_dict = config["refs"]


# Rule-specific arguments
popscle_dict = config["popscle"]
popscle_extra_dict = config["popscle_extra"]
souporcell_dict = config["souporcell"]
souporcell_extra_dict = config["souporcell_extra"]
DoubletDetection_dict = config["DoubletDetection"]
DoubletDetection_manual_dict = config["DoubletDetection_manual"]
DoubletDetection_extra_dict = config["DoubletDetection_extra"]
scrublet_dict = config["scrublet"]
scrublet_manual_dict = config["scrublet_manual"]
scrublet_extra_dict = config["scrublet_extra"]
scds_dict = config["scds"]
CombineResults_dict = config["CombineResults"]



### Get the samples file from Cloud Bucklet ###
client = storage.Client()

bucket = client.get_bucket(workflow.default_remote_prefix)

blob1 = bucket.get_blob(input_dict["samplesheet_filepath"])


##### Read in the samples file that has information for wildcards #####
downloaded_blob1 = blob1.download_as_string()
samples = pd.read_csv(BytesIO(downloaded_blob1), sep = "\t")
samples.columns = ["Pool", "N"]

demuxlet_files = []
souporcell_files = []
scds_files = []
scrublet_files = []
DoubletDetection_files = []
combined_files = []
prepare_files = []

prepare_files.append("file_directories.txt")

include: "includes/prepare_arguments_script.smk"


if bucket.blob("file_directories.txt").exists():

    blob2 = bucket.get_blob("file_directories.txt")
    downloaded_blob2 = blob2.download_as_string()
    scrnaseq_libs_df = pd.read_csv(BytesIO(downloaded_blob2), sep = "\t", index_col = 0)
    

    print(bucket.blob(scrnaseq_libs_df.Matrix_Directories.values[0]).name)



    genes = None
    genes = getGENEfiles(scrnaseq_libs_df)
    print(genes)

    barcodes = None
    barcodes = getBARCODEfiles(scrnaseq_libs_df)
    print(barcodes)

    matrices = None
    matrices = getMATRIXfiles(scrnaseq_libs_df)
    print(matrices)

    if not (genes == None or barcodes == None or matrices == None):


        fasta = getFASTA(ref_dict['ref_dir'])

        if not fasta is None:
            logger.info("using this reference fasta: " + fasta)


            # If the scrublet_check output is present => all the contents are there that are needed to move past 
            if bucket.blob("results/scrublet/scrublet_check.done").exists():

                scrublet_blob = bucket.get_blob("results/manual_selections/scrublet_gene_pctl.txt")
                downloaded_scrublet_blob = scrublet_blob.download_as_string()

                scrublet_decisions = pd.read_csv(BytesIO(downloaded_scrublet_blob), sep = "\t")


            # Import individual rules
            include: "includes/Snakefile_popscle.smk"
            include: "includes/Snakefile_scrublet.smk"
            include: "includes/Snakefile_souporcell.smk"
            include: "includes/Snakefile_scds.smk"
            include: "includes/Snakefile_DoubletDetection.smk"
            include: "includes/Snakefile_CombineResults.smk"



            # Generate list of files to expect
            ## Demuxlet
            demuxlet_files.append(expand("results/{pool}/CombinedResults/demuxlet_results.txt",  pool=samples.Pool))

            ## Souporcell
            souporcell_files.append(expand("results/{pool}/CombinedResults/souporcell_results.txt", pool=samples.Pool))
            souporcell_files.append(expand("results/{pool}/souporcell/Individual_genotypes_subset.vcf.gz", pool=samples.Pool))
            souporcell_files.append(expand("results/{pool}/souporcell/Individual_genotypes_subset.vcf.gz", pool=samples.Pool))

            ## scds
            scds_files.append(expand("results/{pool}/CombinedResults/scds_results.txt", pool=samples.Pool))

            ## scrublet 
            ## the scrublet files that will be run are dependent on user inputs in the yaml file
            scrublet_files.append("results/manual_selections/scrublet/scrublet_percentile_manual_selection.tsv")


            if bucket.blob("results/manual_selections/scrublet/scrublet_percentile_manual_selection.tsv").exists():

                scrublet_selection_blob = bucket.get_blob("results/manual_selections/scrublet/scrublet_percentile_manual_selection.tsv")
                downloaded_scrublet_selection_blob = scrublet_selection_blob.download_as_string()

                scrublet_selection = pd.read_csv(BytesIO(downloaded_scrublet_selection_blob), sep = "\t")
                logger.info("Read in the scrublet manual selection file.")


                if scrublet_selection["scrublet_Percentile"].count() == len(scrublet_selection):

                    logger.info("All the scrublet results have been selected. Will move to next steps.")

                    scrublet_selection["scrublet_Percentile"] = scrublet_selection["scrublet_Percentile"].astype(int)
                    scrublet_files.append(expand("results/{pool}/scrublet_{pctl}/scrublet_results.txt", zip, pool=scrublet_selection.Pool, pctl = scrublet_selection.scrublet_Percentile))


                elif scrublet_selection["scrublet_Percentile"].count() != len(scrublet_selection):

                    logger.info("The scrublets results did not all PASS. Waiting for them to be competed and thresholds selected.")

                    if scrublet_manual_dict["run_scrublet_manual"] == False:

                        logger.info("Running the default scrublet rules since 'run_scrublet_manual' is set to False.")
                        # scrublet_files.append(expand("results/{pool}/scrublet_{pctl}/default_run_variables.txt", pool = samples.Pool, pctl = scrublet_dict["percentile"]))
                        scrublet_files.append(expand("results/{pool}/scrublet_{pctl}/scrublet_results.txt", pool=samples.Pool, pctl =  scrublet_dict["percentile"]))

                    elif scrublet_manual_dict["run_scrublet_manual"] == True:

                        logger.info("Running scrublet rules with manual selections since 'run_scrublet_manual' is set to True.")
                        scrublet_files.append(expand("results/{pool}/scrublet_{pctl}/manual_rerun_variables.txt",zip, pool = scrublet_manual_dict["scrublet_manual_threshold_pools"], pctl = scrublet_manual_dict["scrublet_manual_threshold_percentiles"]))
                        logger.info("Deleting the scrublet results and rerunning since 'run_scrublet_manual' is set to True but results exist.")

                        ### remove the files to force the rule to run if still needing to rerun (will only happen if manual selections aren't filled in all the way)
                        for f in expand("results/{pool}/scrublet_{pctl}/manual_rerun_variables.txt",zip, pool = scrublet_manual_dict["scrublet_manual_threshold_pools"], pctl = scrublet_manual_dict["scrublet_manual_threshold_percentiles"]):

                            fname_blob = bucket.get_blob(f.rstrip())

                            if bucket.blob(fname_blob).exists():

                                fname.delete()

                else:

                    missing_scrublet = set(scrublet_selection['Pool'].astype(str)[[not elem for elem in list(scrublet_selection['scrublet_Percentile'].between(99, 101))]])
                    missing_scrublet_string = "\n".join(missing_scrublet)

                    if len(missing_scrublet) > 0:
                        logger.info("\nERROR: The scrublet_Percentile column in the manual_selections/scrublet/scrublet_percentile_manual_selection.tsv file is not completed.\n\nSpecifically, these individuals do not have appropriate inputs for the scrublet_Percentile column in the 'DoubletDetection_manual_selection.tsv' file:\n" + missing_scrublet_string + "\nPlease fill in these selections for the pipeline to continue.")
                    
                    else:
                        logger.info("\nERROR: The scrublet_Percentile column in the manual_selections/scrublet/scrublet_percentile_manual_selection.tsv file is not completed.\n\nWe were unable to identify the exact pools that contained the issue.\n\nPlease fill in these selections for the pipeline to continue.")


            ## DoubletDetection
            DoubletDetection_files.append("results/manual_selections/DoubletDetection/DoubletDetection_manual_selection.tsv")


            if bucket.blob("results/manual_selections/DoubletDetection/DoubletDetection_manual_selection.tsv").exists():

                DoubletDetection_selection_blob = bucket.get_blob("results/manual_selections/DoubletDetection/DoubletDetection_manual_selection.tsv")
                downloaded_DoubletDetection_selection_blob = DoubletDetection_selection_blob.download_as_string()

                DoubletDetection_selection = pd.read_csv(BytesIO(downloaded_DoubletDetection_selection_blob), sep = "\t")
                logger.info("Read in the DoubletDetection manual selection file.")


                if len(DoubletDetection_selection[DoubletDetection_selection['DoubletDetection_PASS_FAIL'].astype(str).str.contains('PASS', na=False)]) == len(DoubletDetection_selection):

                    logger.info("All the DoubletDetection results have PASSED. Will move to next steps.")
                    DoubletDetection_files.append(expand("results/{pool}/CombinedResults/DoubletDetection_results.txt", pool=DoubletDetection_selection.Pool))


                elif len(DoubletDetection_selection[DoubletDetection_selection['DoubletDetection_PASS_FAIL'].astype(str).str.contains('PASS', na=False)]) != len(DoubletDetection_selection):

                    logger.info("The DoubletDetection results did not all PASS. Waiting for them to be competed and PASS.")


                    if DoubletDetection_manual_dict["run_DoubletDetection_manual"] == False:

                        logger.info("Running the default DoubletDetection rules since 'run_DoubletDetection_manual' is set to False.")
                        DoubletDetection_files.append(expand("results/{pool}/DoubletDetection/default_run_variables.txt", pool = samples.Pool))


                    elif DoubletDetection_manual_dict["run_DoubletDetection_manual"] == True:

                        logger.info("Running DoubletDetection for specified rules since 'run_DoubletDetection_manual' is set to True.")
                        DoubletDetection_files.append(expand("results/{pool}/DoubletDetection/manual_rerun_variables.txt", pool = DoubletDetection_manual_dict["DoubletDetection_manual_pools"]))
                        logger.info("Deleting the DoubletDetection results and rerunning since 'run_DoubletDetection_manual' is set to True but results exist.")

                        ### remove the files to force the rule to run if still needing to rerun (will only happen if manual selections aren't filled in all the way)
                        for f in expand("results/{pool}/DoubletDetection/manual_rerun_variables.txt", pool = DoubletDetection_manual_dict["DoubletDetection_manual_pools"]):

                            fname_blob = bucket.get_blob(f.rstrip())

                            if bucket.blob(fname_blob).exists():
                            
                                fname.delete()

                                
                else:

                    missing_DoubletDetection = set(DoubletDetection_selection['Pool'].astype(str)[[not elem for elem in list(DoubletDetection_selection['DoubletDetection_PASS_FAIL'].astype(str).isin(pd.Series(['PASS', 'FAIL'])))]])
                    missing_DoubletDetection_string = "\n".join(missing_DoubletDetection)


                    if len(missing_DoubletDetection) > 0:
                        logger.info("\nERROR: The DoubletDetection_PASS_FAIL column in the manual_selections/DoubletDetection/DoubletDetection_manual_selection.tsv file is not completed.\n\nSpecifically, these individuals do not have appropriate inputs for the DoubletDetection_PASS_FAIL column in the 'DoubletDetection_manual_selection.tsv' file: " + missing_DoubletDetection_string + "\nPlease fill in these selections for the pipeline to continue.")
                    
                    else:
                        logger.info("\nERROR: The DoubletDetection_PASS_FAIL column in the manual_selections/DoubletDetection/DoubletDetection_manual_selection.tsv file is not completed.\n\nWe were unable to identify the exact pools that contained the issue.\n\nPlease fill in these selections for the pipeline to continue.")


            ## Combined files at the end

            if bucket.blob("results/manual_selections/scrublet/scrublet_percentile_manual_selection.tsv").exists() and bucket.blob("results/manual_selections/DoubletDetection/DoubletDetection_manual_selection.tsv").exists():

                logger.info("Found the 'scrublet_percentile_manual_selection.tsv' and DoubletDetection_manual_selection.tsv files. Reading in.")

                if scrublet_selection["scrublet_Percentile"].count() == len(scrublet_selection) and len(DoubletDetection_selection[DoubletDetection_selection['DoubletDetection_PASS_FAIL'].astype(str).str.contains('PASS', na=False)]) == len(DoubletDetection_selection):

                    logger.info("All the DoubletDetection results have PASSED and thresholds selected for scrublet. Will move to next steps.")
                    combined_files.append(expand("results/{pool}/CombinedResults/Final_Assignments_demultiplexing_doublets.txt", pool = samples.Pool))
                    combined_files.append("results/QC_figures/UMI_vs_Genes_QC_scatter.png")
                    combined_files.append("results/QC_figures/expected_observed_individuals_classifications.png")


                else:

                    logger.info("The DoubletDetection and scrublet results were not complete. Once they are complete, the results can be combined and downstream analyses completed.")

        else:
            logger.info("Could not find the necessary fasta file.\n\n Exiting.")

    else:
        logger.info("We're having issues finding your gene, barcod or matrix file. Please make sure these exist in " + bucket.blob(scrnaseq_libs_df.Matrix_Directories.values[0]).name)

else:
    logger.info("making file_directoreies.")


# Main rule - input will be all files generated at the top
rule all:
    input:
        demuxlet_files,
        souporcell_files,
        scds_files,
        scrublet_files,
        DoubletDetection_files,
        combined_files,
        prepare_files